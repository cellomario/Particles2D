 
:: initializing oneAPI environment ...
   bash: BASH_VERSION = 5.1.16(1)-release
   args: Using "$@" for setvars.sh arguments: intel64 --force
:: compiler -- latest
:: debugger -- latest
:: dev-utilities -- latest
:: tbb -- latest
:: vtune -- latest
:: oneAPI environment initialized ::
 
Starting at: Thu Aug 24 14:58:49 2023
Initializing grids ...
MaxIters = 3000
MaxSteps = 10
TimeBit = 0.001000
GenFieldGrid i2dGrid: EX, EY = 2400, 2000
         Xs, Xe = 0.427734, 0.451465; Ys, Ye = 0.321289, 0.342053
ParticleGrid i2dGrid: EX, EY = 1200, 1000
         Xs, Xe = 0.000000, 1200.000000; Ys, Ye = 0.000000, 1000.000000
GeneratingField...
Computing generating field ...
ParticleGeneration...
Population: np = 21352
SystemEvolution...
Step 0 of 10
Step 1 of 10
Step 2 of 10
Step 3 of 10
Step 4 of 10
Step 5 of 10
Step 6 of 10
Step 7 of 10
Step 8 of 10
Step 9 of 10
Ending   at: Thu Aug 24 14:59:03 2023
Computations ended in 14.000000 seconds
End of program!
Elapsed Time: 13.259s
    SP GFLOPS: 0.000
    DP GFLOPS: 4.669
    x87 GFLOPS: 0.513
    CPI Rate: 0.983
    Average CPU Frequency: 3.067 GHz
    Total Thread Count: 46
Effective Physical Core Utilization: 78.0% (3.122 out of 4)
 | The metric value is low, which may signal a poor physical CPU cores
 | utilization caused by:
 |     - load imbalance
 |     - threading runtime overhead
 |     - contended synchronization
 |     - thread/process underutilization
 |     - incorrect affinity that utilizes logical cores instead of physical
 |       cores
 | Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism
 | or run the Locks and Waits analysis to identify parallel bottlenecks for
 | other parallel runtimes.
 |
    Effective Logical Core Utilization: 58.7% (4.699 out of 8)
     | The metric value is low, which may signal a poor logical CPU cores
     | utilization. Consider improving physical core utilization as the first
     | step and then look at opportunities to utilize logical cores, which in
     | some cases can improve processor throughput and overall performance of
     | multi-threaded applications.
     |
    Serial Time (outside parallel regions): 1.118s (8.4%)

        Top Serial Hotspots (outside parallel regions)
        Function                                  Module               Serial CPU Time
        ----------------------------------------  -------------------  ---------------
        [Loop at line 0 in GeneratingField]       particles2023_c.exe           0.569s
        [Loop at line 847 in IntVal2ppmB]         particles2023_c.exe           0.041s
        [Loop at line 505 in ParticleGeneration]  particles2023_c.exe           0.020s
        [Loop at line 488 in ParticleGeneration]  particles2023_c.exe           0.019s
        [Loop at line 0 in GeneratingField]       particles2023_c.exe           0.017s
        [Others]                                  N/A                           0.076s
    Parallel Region Time: 12.141s (91.6%)
        Estimated Ideal Time: 10.317s (77.8%)
        OpenMP Potential Gain: 1.824s (13.8%)
         | The time wasted on load imbalance or parallel work arrangement is
         | significant and negatively impacts the application performance and
         | scalability. Explore OpenMP regions with the highest metric values.
         | Make sure the workload of the regions is enough and the loop schedule
         | is optimal.
         |
Memory Bound: 20.8% of Pipeline Slots
 | The metric value is high. This can indicate that the significant fraction of
 | execution pipeline slots could be stalled due to demand memory load and
 | stores. Use Memory Access analysis to have the metric breakdown by memory
 | hierarchy, memory bandwidth information, correlation by memory objects.
 |
    Cache Bound: 22.7% of Clockticks
     | A significant proportion of cycles are being spent on data fetches from
     | caches. Check Memory Access analysis to see if accesses to L2 or L3
     | caches are problematic and consider applying the same performance tuning
     | as you would for a cache-missing workload. This may include reducing the
     | data working set size, improving data access locality, blocking or
     | partitioning the working set to fit in the lower cache levels, or
     | exploiting hardware prefetchers. Consider using software prefetchers, but
     | note that they can interfere with normal loads, increase latency, and
     | increase pressure on the memory system. This metric includes coherence
     | penalties for shared data. Check Microarchitecture Exploration analysis
     | to see if contested accesses or data sharing are indicated as likely
     | issues.
     |
    DRAM Bound: 0.0% of Clockticks
        DRAM Bandwidth Bound: 0.9% of Elapsed Time

    Bandwidth Utilization
    Bandwidth Domain  Platform Maximum  Observed Maximum  Average  % of Elapsed Time with High BW Utilization(%)
    ----------------  ----------------  ----------------  -------  ---------------------------------------------
    DRAM, GB/sec      11                          10.900    0.692                                           0.9%
Vectorization: 35.1% of Packed FP Operations
 | A significant fraction of floating point arithmetic instructions are scalar.
 | This indicates that the code was not fully vectorized. Use Intel Advisor to
 | see possible reasons why the code was not vectorized.
 |
    Instruction Mix
        SP FLOPs: 0.0% of uOps
            Packed: 0.0% from SP FP
                128-bit: 0.0% from SP FP
                256-bit: 0.0% from SP FP
            Scalar: 0.0% from SP FP
        DP FLOPs: 20.4% of uOps
            Packed: 40.5% from DP FP
                128-bit: 40.5% from DP FP
                 | Using the latest vector instruction set can improve
                 | parallelism for this code. Consider either recompiling the
                 | code with the latest instruction set or using Intel Advisor
                 | to get vectorization help.
                 |
                256-bit: 0.0% from DP FP
            Scalar: 59.5% from DP FP
             | A significant fraction of floating point arithmetic instructions
             | are scalar. This indicates that the code was not fully
             | vectorized. Use Intel Advisor to see possible reasons why the
             | code was not vectorized.
             |
        x87 FLOPs: 3.1% of uOps
        Non-FP: 76.5% of uOps
    FP Arith/Mem Rd Instr. Ratio: 1.308
    FP Arith/Mem Wr Instr. Ratio: 11.363
Collection and Platform Info
    Application Command Line: ./particles2023_c.exe 
    Operating System: 6.2.0-26-generic DISTRIB_ID=Ubuntu DISTRIB_RELEASE=22.04 DISTRIB_CODENAME=jammy DISTRIB_DESCRIPTION="Ubuntu 22.04.3 LTS"
    Computer Name: cellomarioThinkPad
    Result Size: 88,3 MB 
    Collection start time: 12:58:49 24/08/2023 UTC
    Collection stop time: 12:59:03 24/08/2023 UTC
    Collector Type: Driverless Perf per-process sampling
    CPU
        Name: Intel(R) microarchitecture code named Cometlake U
        Frequency: 2.304 GHz
        Logical CPU Count: 8
        Max DRAM Single-Package Bandwidth: 11.000 GB/s

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
